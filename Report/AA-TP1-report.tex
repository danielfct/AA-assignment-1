\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
%% To use hyperlinks
\usepackage{hyperref}
%% To use colors
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

%%To use maths
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
\usepackage{titlesec}

\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}} 

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}


\newcommand\tab[1][1cm]{\hspace*{#1}}

\title{\textbf{Aprendizagem AutomÃ¡tica} \\
\large Assignment 1 - Classifiers}

\author{Andrea Mascaretti N52222\and Daniel Pimenta N45404}

\begin{document}
\maketitle

\begin{abstract}
The main goal of this assignment was the parameterization, fitting and comparation of Logistic Regression,
 K-nearest Neighbours and Naive Bayes classifiers.\\ 
The data set used was the banknote authentication which was obtained from the
 \href{https://archive.ics.uci.edu/ml/datasets/banknote+authentication}{UCI machine learning repository}.\\
To achieve our goals we used the Spyder IDE with programming language Python 3.6 and some of its modules 
such as \textit{Panda} to load and manage our data, \textit{NumPy} which is the fundamental package for 
scientific computing with Python and was used for various mathematical calculations and managing multidimensional 
arrays, \textit{sklearn (scikit-learn)} which has efficient tools for data mining and data analysis used in our assignment 
to preprocess data (shuffle, standardize, split), fold our data into k stratified folds, calculate cross validation score and 
fit our data on Logistic Regression and K-nearest Neighbours classifiers. Another imported module was Matplotlib.Pyplot
 which provides a MATLAB-like plotting framework used to build the error plots. 
\end{abstract}

\section{Introduction}
Using the Spyder IDE and programming language Python 3.6 and with a csv file which held data about bank notes authenticity 
we were asked to parameterize, fit and compare three different classifiers: Logistic Regression, K-nearest Neighbours and Naive Bayes. 
For the first two classifiers, we were allowed to import them from a module which was provided by scikit-learn. 
The Naive Bayes classifier was fully implemented for our assignment to fit our data using the best bandwidth we could find and to then later predict the real values
for a test set.\\
The initial data was split into a training set and a test set. The training set was then folded into 5 folds to be used by Cross Validation.
For the Logistic Regression classifier we started with a 'c' value of 1 and incremented to its double for 20 iterations.
For the k value of the K-Nearest Neighbours classifier, we tested k values from 1 to 39 using odd values only. 
In the end, we hope to compare our three classifiers performance using McNemar's test with 95\%  confidence interval 
by comparing their predictions with the real values and pick the best classifier approached in this assignment.

\section{Classifiers}

\subsection{Logistic Regression}

The logistic regression is a generalised linear model. Before considering
a more classic machine learning framework, we provide a brief introduction
to the main idea behind the model. 

The logistic regression is composed, as every linear model, of three
part:
\begin{itemize}
	\item A sequence of \textit{response variables} $Y_{1},\,\ldots,\,Y_{n}$.
	Such response variables are called random components. The main assumption
	we shall make regarding those variables is that they are all independent
	random variables. Moreover, each one of them will have a distribution
	from the exponential family. Bear in mind that we are not imposing
	that the various $Y_{i}$s are identically distributed.
	\item The \textit{systematic component} is our model. It is a function of
	some predictors (also known as regressors or covariates), linear in
	the parameter and related to the mean of $Y_{i}$.
	\item The \textit{link function} $g\left(\mu_{i}\right)$ will allow us
	to link the two components, allowing us to say that 
	\begin{equation}
	g\left(\mu_{i}\right)=\beta_{0}+\sum_{i=1}^{r}\beta_{i}x_{i}\label{eq:-3}
	\end{equation}
	where $\mu_{i}=\mathbb{E}\left[Y_{i}\right]$. In the machine learning
	framework, it is usually more common to consider the inverse of the
	link function.
\end{itemize}
In our model we will assume the following:
\begin{equation}
Y_{i}\overset{ind}{\sim}\mathsf{Bernoulli}\left(\pi_{i}\right)\label{eq:}
\end{equation}
and therefore we will have that 
\begin{equation}
\mathbb{E}\left[Y_{i}\right]=\pi_{i}.\label{eq:-1}
\end{equation}
Now, in this model we will assume a relationship between the logarithm
of the odds of success for $Y_{i}$ (the log odds or \textit{logit})
and the predictor $\mathbf{x}=\left(1,\,x_{1},\,\ldots,\,x_{r}\right)$.
Mathematically, this entails
\begin{equation}
\ln\left(\frac{\pi}{1-\pi}\right)=\mathbf{\beta^{\prime}\mathbf{x}}\label{eq:-6}
\end{equation}
and $\mathbf{\beta}=\left(\beta_{0},\,\ldots,\,\beta_{r}\right)$.

Rewriting in order to obtain the exponential family form we have
\begin{equation}
\pi^{y}\left(1-\pi\right)^{1-y}=\left(1-\pi\right)\exp\left\{ y\,\ln\left(\frac{\pi}{1-\pi}\right)\right\} ,\label{eq:-2}
\end{equation}
where the term $\ln\left(\frac{\pi}{1-\pi}\right)$ is the natural
parameter of the exponential family and shows why we will implement
the link function
\begin{equation}
g\left(\pi\right)=\ln\left(\frac{\pi}{1-\pi}\right).\label{eq:-4}
\end{equation}

Rewriting \ref{eq:-6}, we obtain
\begin{equation}
\pi\left(x\right)=\frac{\exp\left\{ \mathbf{\beta}^{\prime}\mathbf{x}\right\} }{1+\exp\left\{ \mathbf{\beta}^{\prime}\mathbf{x}\right\} }.\label{eq:-5}
\end{equation}

What we have written is equivalent to saying that 
\begin{equation}
\mathsf{p}\left(\mathcal{C}_{1}|\,\mathbf{x}\right)=\pi\left(\mathbf{x}\right)=\sigma\left(\mathbf{\beta}^{\prime}\mathbf{x}\right)\label{eq:-7}
\end{equation}
where $\sigma\left(\cdot\right)$ is the logistic sigmoid and $\mathcal{C}_{1}$
is to identify class $1$. Notice that it holds $\mathsf{p}\left(\mathcal{C}_{2}|\,\mathbf{x}\right)=1-\mathsf{p}\left(\mathcal{C}_{1}|\,\mathbf{x}\right)=1-\sigma\left(\mathbf{\beta}^{\prime}\mathbf{x}\right)$.

Suppose we are given a dataset $\left\{ \mathbf{x}_{i},\,y_{i}\right\} $
where now $y_{i}\in\left\{ -1,\,1\right\} $ and $\mathbf{x}_{i}\in\mathbb{R}^{r}$
for $i=1,\,\ldots,\,n$. Given our hypotheses, we can write the negative
log-likelihood function as
\begin{equation}
\sum_{i=1}^{n}\ln\left(1+\exp\left\{ -y_{i}\,\left(\mathbf{\beta}^{\prime}\mathbf{x}_{i}\right)\right\} \right).\label{eq:-8}
\end{equation}
To this term we added a $\mathsf{L2}$ penalization term to obtain
a more regularized result, preferring this to its $\mathsf{L1}$ version
as the latter usually provides sparser solutions and we were working
in an environment with only four covariates. This lead to the following
problem:
\begin{equation}
\underset{\mathbf{\beta}}{\min\,}f\left(\mathbf{\beta}\right)=\frac{1}{2}\mathbf{\beta}^{\prime}\mathbf{\beta}+C\,\sum_{i=1}^{n}\ln\left(1+\exp\left\{ -y_{i}\,\left(\mathbf{\beta}^{\prime}\mathbf{x}_{i}\right)\right\} \right),\label{eq:-9}
\end{equation}
where $C\,>\,0$ is a parameter that can be assigned in order to balance
the two terms. To tune it, we relied on a stratified 5-fold cross-validation
on the training set, picking the value that yielded the lowest value.
From a numerical point of view, the trust region Newton method built
in the $\mathsf{ScikitLearn}$ class for $\mathsf{L2}$-logistic regression
was used.

\subsubsection{Optimal C}

Optimal C value found: 256\\
At Cross Validation error value: 0.011\\

// TODO\\
explain what is the c value and the effect on its classifier and how we got the optimal value

%\includegraphics{Best C value - L2 Logistic Regression.png}

\subsection{k-Nearest Neighbours}

Suppose that we have some classes $\mathcal{C}_{k}$ such that each
class contains $N_{k}$ points and let $N=\sum_{k}N_{k}$ be the total
number of points. Let us suppose that we are also given a distance
function $\rho:\,\mathbb{R}^{r}\rightarrow\mathbb{R}^{+}$. To classify
a new data point $\mathbf{x}$, we draw a sphere (according to $\rho$)
such that exactly $k$ points, regardless of their class, fall into
it. If we let $V$ be the volume of such sphere and $K_{k}$ be the
number of points of class $k$ contained in it, we see that 
\begin{equation}
\mathsf{p}\left(\mathbf{x}|\,\mathcal{C}_{k}\right)=\frac{K_{k}}{N_{k}V},\label{eq:-10}
\end{equation}
the marginal probability is given by 
\begin{equation}
\mathsf{p}\left(\mathbf{x}\right)=\frac{K}{NV}\label{eq:-11}
\end{equation}
and the prior distributions are 
\begin{equation}
\mathsf{p}\left(\mathcal{C}_{k}\right)=\frac{N_{k}}{N}.\label{eq:-12}
\end{equation}

Therefore, applying Bayes' theorem we have 
\begin{equation}
\mathsf{p}\left(\mathcal{C}_{k}|\mathbf{x}\right)=\frac{\mathsf{p}\left(\mathbf{x}|\,\mathcal{C}_{k}\right)\mathsf{p}\left(\mathcal{C}_{k}\right)}{\mathsf{p}\left(\mathbf{x}\right)}=\frac{K_{k}}{K}.\label{eq:-13}
\end{equation}
Comparing the postierior probabilities, we classified each data picking
the maximum \textit{a posteriori}. We consider the usual Euclidean
distance for our distance function and trained only for odds value
of $k$ (between $1$ and $39$), picking the one that yielded the
best result on a Stratified $5$-fold cross validation on our data
set.

\subsubsection{Optimal K}

Optimal K value found: 5\\
At Cross Validation error value: 0.002\\

// TODO\\
explain what is the k value and the effect on its classifier and how we got the optimal value

%\includegraphics{Best K value - K-nearest Neighbours.png}

\subsection{Kernel Naive Bayes Classifier}
Let $\mathcal{C}_{k}$ be a class and let $\mathbf{x}$ our data vector.
As done before, we can apply Bayes' theorem to provide the \textit{a
	posteriori} probability of belonging to such class by stating \ref{eq:-13}.
Naive Bayes' classifier is based on the hypothesis that the attributes
of our data are conditionally independent, allowing us to rewrite
the conditional likelihood as 
\begin{equation}
\mathsf{p}\left(\mathbf{x}|\,\mathcal{C}_{k}\right)=\prod_{i=1}^{r}\mathsf{p}\left(x_{i}|\,\mathcal{C}_{k}\right).\label{eq:-14}
\end{equation}
This assumption, that is in fact \textit{naive}, does not usually
hurt too much as we are interested in picking the maximum probability
a\textit{ posteriori}.

Naive Bayes is usually implemented by considering a probability density
function, that is fitted to the feature being taken into account.
In this particular instance, however, the conditional likelihood was
modelled applying kernel density estimation techniques.

If we let $\Omega$ be a nonempty set, a symmetric function (\textit{i.e.}
a function that attains the same value for any permutation of its
arguments) $K:\,\Omega\times\Omega\rightarrow\mathbb{R}$ is called
a positive definite kernel on $\Omega$ if 
\begin{equation}
\sum\sum c_{i}c_{j}K\left(x_{i},\,x_{j}\right)\geq0\label{eq:-15}
\end{equation}
for any $n\in\mathbb{N},\,x_{1},\,\ldots,\,x_{n}\in\Omega,\,c_{1},\,\ldots,\,c_{n}\in\mathbb{R}$.

In this specific setting, we considered the RBF Kernel (also known
as Gaussian kernel), of the form 
\begin{equation}
K_{h}\left(\mathbf{x},\mathbf{x}\right)=\exp\left\{ -\frac{\Vert\mathbf{x}-\mathbf{x^{\prime}}\Vert}{h}\right\} .\label{eq:-16}
\end{equation}

Once the kernel form is fixed, the density for a point $y$ can be
computed by considering 
\[
\rho_{K}\left(y\right)=\sum K\left(\left(y-x_{i}\right)/h\right).
\]

Notice that the bandwidth here acts as a bias-variance trade-off parameter.

To classify the data we considered the logarithm of the posterior
probability, obtaining

\begin{equation}
\ln\left(\mathsf{p}\left(\mathcal{C}_{k}|\mathbf{x}\right)\right)\propto\ln\left(\mathsf{p}\left(\mathcal{C}_{k}\right)\right)+\sum\ln\left(\mathsf{p}\left(\mathbf{x}|\mathcal{C}_{k}\right)\right).\label{eq:-17}
\end{equation}
The prior was fixed simply by taking into accounting the elements
per class and the likelihood by means of the $\mathsf{KernelDensity}$
function in the $\mathsf{ScikitLearn}$ library.

To classify our data into the two categories, we considered 
\begin{equation}
\arg\underset{k\in\left\{ 0,1\right\} }{\max}\ln\left(\mathsf{p}\left(\mathcal{C}_{k}\right)\right)+\sum\ln\left(\mathsf{p}\left(\mathbf{x}|\mathcal{C}_{k}\right)\right).\label{eq:-18}
\end{equation}
To set the parameter $h$, once again a 5-fold stratified cross validation
was implemented.

\subsubsection{Optimal Bandwidth found}

Optimal Bandwidth found: 0.05\\
At Cross Validation error value: 0.00\\

// TODO\\
explain what is the bandwidth and the effect on its classifier and how we got the optimal value

%\includegraphics{Best Bandwidth - Naive Bayes.png}





// TODO\\
Explain the method by which the optimal values were found, noting
the differences between the errors and the importance of leaving out
a test set for the final evaluation\\





\section{Comparing Classifiers}

After parameterizing and fitting our train data into our classifiers we used our test data to calculate the true error of all
three classifiers.

\subsection{True Error}

\subsubsection{Logisitic Regression}
	\tab True Negative: 253\\
        	\tab False Positive: 1\\
        	\tab False Negative: 2\\
        	\tab True Positive: 201\\
        	\tab Test Error: 0.0066

\subsubsection{K-nearest Neighbours}
	\tab True Negative: 254\\
        	\tab False Positive: 0\\
        	\tab False Negative: 0\\
        	\tab True Positive: 203\\
        	\tab Test Error: 0.0000

\subsubsection{Naive Bayes}
TODO
	\tab True Negative: 240\\
        	\tab False Positive: 14\\
        	\tab False Negative: 73\\
        	\tab True Positive: 130\\
        	\tab Test Error: 0.1904

\bigbreak
For comparing the three classifiers performances, we used McNemar's test with a 95\% confidence interval.

\subsection{McNemar's test (with continuity correction)}
Let e01 be the number of examples the first classifier
misclassifies but the second classifies correctly, and e10 be the number of examples the second classifier
classifies incorrectly but the first classifier classifies correctly. The difference divided by the total
follows approximately a chi-squared distribution with one degree of freedom:

\begin{equation}
\frac{(|e_{01} - e_{10}| - 1)^2}{e_{01}+e_{10}}\approx\rchi^2_{1}
\end{equation}

From the predicted classification values of two classifiers we were able to calculate the e01 and e10 values by 
doing a simple loop and comparing both classifications with the real classification value:\\

\begin{algorithm}[H]
 	\KwData{first predictions, second predictions, real values}
 	\KwResult{McNemar's test}
	initialize e01 and e10\;
 	\For{every value in real values}{
		get current value of first predictions\;
		get current value of second predictions\;
  		\If{\(\lvert currentFirstPrediction - value \rvert\) equals 1 and second prediction - value equals 0}{
			increment e01\;
   		}
   		\If{first prediction - value equals 0 and \(\lvert currentSecondPrediction - value \rvert\) equals 1}{
			increment e10\;
   		}
  	}
	\Return $\frac{(|e_{01} - e_{10}| - 1)^2}{e_{01}+e_{10}}$
	\caption{McNemar's test}
\end{algorithm}
\bigbreak

After running the McNemar's test to compare all three classifiers we got this result:\\
\begin{center}
Logistic Regression VS K-nearest neighbours: 1.333\\
Logistic Regression VS Naive Bayes: 1.333\\
K-nearest neighbours VS Naive Bayes: Not applicable
TODO
\end{center}

Which indicates that Logistic Regression and K-nearest neighbours classifiers have similar performance. 
Also Logistic Regression and Naive Bayes have a similar performance.
The comparation of K-nearest neighbours and Naive Bayes was not possible with McNemar's test because
they both achieved a 0\% test error. But that should indicate both classifiers have similar performaces.

\section{Discussion and Conclusion}
This assignment aimed to help us understand the fundamental principles of three 
classifiers: Logistic Regression, K-nearest Neighbours and Naive Bayes. 
It also allowed us to understand and apply some data management techniques 
such as preprocessing data (Load, Shuffle, Split) and the process of Cross Validation. 
The first step on this assignment was to load and preprocessing the data from a 
file which held data from the UCI machine learning repository about bank notes 
authenticity. The goal of the three classifiers was to predict either a bank note 
was real of fake based on four features (Variance, Skewness, Curtosis, Entropy).
A test of performance of those classifiers was done to compare and eventually 
choose the best and to discuss if any is significantly better than the others. 
From the McNemar's tests we noticed that Logistic Regression and K-nearest 
Neighbours classifiers performances were similar, the latter being just 
a bit better, but not enough to differentiate their performances.
  

\section{Bibliography}
\begin{itemize}
\item \href{http://aa.ssdi.di.fct.unl.pt/files/AA-05.pdf}{Lecture 5}, 
	\href{http://aa.ssdi.di.fct.unl.pt/files/AA-05_notes.pdf}{Lecture 5 Notes}, 
	\href{http://aa.ssdi.di.fct.unl.pt/files/AA-06.pdf}{Lecture 6}, 
	\href{http://aa.ssdi.di.fct.unl.pt/files/AA-06_notes.pdf}{Lecture 6 Notes}, 
	\href{http://aa.ssdi.di.fct.unl.pt/files/AA-07.pdf}{Lecture 7}, 
	\href{http://aa.ssdi.di.fct.unl.pt/files/AA-07_notes.pdf}{Lecture 7 Notes}\\
\item \href{http://aa.ssdi.di.fct.unl.pt/files/Tutorials.pdf}{Tutorial Notes}\\
\item \href{http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing}{Scikit-learn Preprocessing docs}\\
\item \href{http://scikit-learn.org/stable/supervised_learning.html#supervised-learning}{Scikit-learn Supervised Learning docs}\\
\item \href{http://scikit-learn.org/stable/model_selection.html#model-selection}{Scikit-learn Model Selection docs}\\
\end{itemize}

\end{document}